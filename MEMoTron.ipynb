{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylerle29/Memotron/blob/main/MEMoTron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blQrFsAXcPbn",
        "outputId": "feb31816-84e5-4f9a-90a1-73c5929413b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Llama-3.1-Nemotron-Nano-4B-v1.1' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU vllm"
      ],
      "metadata": {
        "id": "ecUhxn_lcrXj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model Llama-3.1-Nemotron-Nano-4B-v1.1 \\\n",
        "  --trust-remote-code \\\n",
        "  --seed 1 \\\n",
        "  --host \"0.0.0.0\" \\\n",
        "  --port 5000 \\\n",
        "  --served-model-name \"Llama-Nemotron-Nano-4B-v1.1\" \\\n",
        "  --tensor-parallel-size 1 \\\n",
        "  --max-model-len 50_000 \\\n",
        "  --gpu-memory-utilization 0.95 \\\n",
        "  --enforce-eager \\\n",
        "  --enable-auto-tool-choice \\\n",
        "  --tool-parser-plugin \"Llama-3.1-Nemotron-Nano-4B-v1.1/llama_nemotron_nano_toolcall_parser.py\" \\\n",
        "  --tool-call-parser \"llama_nemotron_json\" \\\n",
        "  --chat-template \"Llama-3.1-Nemotron-Nano-4B-v1.1/llama_nemotron_nano_generic_tool_calling.jinja\" \\\n",
        "  > vllm.log &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMVwZCTZdsSG",
        "outputId": "37008451-16d9-471a-94d9-e40a7f71a8ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!while ! grep -q \"Application startup complete\" vllm.log; do tail -n 1 vllm.log; sleep 5; done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnaEKAPTdtIc",
        "outputId": "bc00e733-321e-4bd7-caeb-a6ac5cf2afc6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-09 12:15:26.610240: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[1;36m(APIServer pid=10344)\u001b[0;0m INFO 11-09 12:15:35 [model.py:1510] Using max model len 50000\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors\n",
        "!pip install flask pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2vCVzRvt2I5",
        "outputId": "64d60867-e363-42a2-a665-feffe9754798"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.12/dist-packages (6.0.1)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.12/dist-packages (from flask-cors) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.12/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (3.0.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import quote\n",
        "import re\n",
        "\n",
        "def kym_search_image(query):\n",
        "    url = f\"https://knowyourmeme.com/search?q={quote(query)}\"\n",
        "    r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "    # 1. find the first meme-page link that contains an image with data-image\n",
        "    candidates = soup.select('a[href^=\"/memes/\"]')\n",
        "    page = None\n",
        "\n",
        "    for a in candidates:\n",
        "        img = a.select_one(\"img[data-image]\")\n",
        "        if img:\n",
        "            page = a\n",
        "            img_tag = img\n",
        "            break\n",
        "\n",
        "    if page is None:\n",
        "        print(\"No match\")\n",
        "        return None\n",
        "\n",
        "    # page URL\n",
        "    page_url = \"https://knowyourmeme.com\" + page[\"href\"]\n",
        "\n",
        "    # high-quality image from data-image\n",
        "    img_url = img_tag[\"data-image\"]\n",
        "\n",
        "    print(\"Page:\", page_url)\n",
        "    print(\"Image:\", img_url)\n",
        "    return (page_url, img_url)\n",
        "\n",
        "def scrape_text_kym(url):\n",
        "\n",
        "    # 1. Request webpage\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    response.raise_for_status()\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Parse HTML\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Sets up the sections we need to parse.\n",
        "\n",
        "    sections = {}\n",
        "\n",
        "    relevant_titles = [\n",
        "\n",
        "    \"About\", \"Origin\", \"Spread\", \"History\", \"Background\", \"Meaning\", \"Usage\", \"Development\", \"Overview\", \"External References\"\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    # Parses them.\n",
        "\n",
        "    for h2 in soup.find_all(\"h2\"):\n",
        "\n",
        "        title = h2.get_text(strip=True)\n",
        "\n",
        "        if title in relevant_titles:\n",
        "\n",
        "            content = []\n",
        "\n",
        "\n",
        "\n",
        "            # Walk through siblings until the next h2\n",
        "\n",
        "            for sib in h2.next_siblings:\n",
        "\n",
        "                if getattr(sib, \"name\", None) == \"h2\":\n",
        "\n",
        "                    break\n",
        "\n",
        "                if sib.name in [\"p\"]:\n",
        "\n",
        "                    text = sib.get_text(\" \", strip=True)\n",
        "\n",
        "                    if text:\n",
        "                        text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "                        content.append(text.strip())\n",
        "\n",
        "\n",
        "\n",
        "            sections[title] = \"\\n\".join(content)\n",
        "\n",
        "\n",
        "\n",
        "    return sections\n",
        "\n",
        "def scrape_visible_text(url, timeout=10):\n",
        "    r = requests.get(url, timeout=timeout, headers={\n",
        "        \"User-Agent\": \"Mozilla/5.0\"\n",
        "    })\n",
        "    r.raise_for_status()\n",
        "    html = r.text\n",
        "    # remove all HTML tags\n",
        "    text = re.sub(r\"<[^>]+>\", \" \", html)\n",
        "    # collapse whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def scrape_all_paragraphs(url):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  response.raise_for_status()\n",
        "\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  paragraphs = []\n",
        "  for p in soup.find_all(\"p\"):\n",
        "      text = p.get_text(\" \", strip=True)\n",
        "      if text:\n",
        "          paragraphs.append(text)\n",
        "\n",
        "  return \"\\n\".join(paragraphs)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "S6kyQizhxu95"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "conversation = \"\"\n",
        "conv_lock = threading.Lock()\n",
        "\n",
        "client = OpenAI(base_url=\"http://127.0.0.1:5000/v1\", api_key=\"dummy\")\n",
        "\n",
        "@app.route(\"/api/respond\", methods=[\"POST\"])\n",
        "def respond():\n",
        "    global conversation\n",
        "    data = request.json\n",
        "    user_input = data.get(\"input\", \"\")\n",
        "\n",
        "    page_url, img_url = kym_search_image(user_input)\n",
        "    sections = scrape_text_kym(page_url)\n",
        "\n",
        "    context = \"\"\n",
        "    for title, text in sections.items():\n",
        "        context += f\"{title}: {text}\\n\"\n",
        "\n",
        "    buffer = context.strip()\n",
        "    max_chars = 2500 + len(context)\n",
        "\n",
        "    while len(buffer) < max_chars:\n",
        "        prompt = (\n",
        "            \"Given the following context, propose 3â€“5 broad google search terms \"\n",
        "            \"that would expand the context. Return ONLY a JSON array of strings.\\n\\n\"\n",
        "            f\"CONTEXT:\\n{buffer[:2000]}\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Be precise. Output JSON only.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "            messages=messages,\n",
        "            temperature=0.4,\n",
        "            stream=False\n",
        "        )\n",
        "\n",
        "        raw_terms = resp.choices[0].message.content\n",
        "        try:\n",
        "            import json\n",
        "            terms = json.loads(raw_terms)\n",
        "            if not isinstance(terms, list):\n",
        "                break\n",
        "        except:\n",
        "            break\n",
        "\n",
        "        for term in terms:\n",
        "            google_page = f\"https://www.google.com/search?q={term.replace(' ', '+')}\"\n",
        "            scraped = scrape_visible_text(google_page)\n",
        "            if scraped:\n",
        "                buffer += \"\\n\" + scraped\n",
        "                if len(buffer) >= max_chars:\n",
        "                    break\n",
        "\n",
        "        compression_prompt = (\n",
        "            \"Summarize the important points of the following text. \"\n",
        "            \"Be succinct and do not exceed 2 paragraphs.\\n\\n\"\n",
        "            f\"{buffer[-3000:]}\"\n",
        "        )\n",
        "        comp_resp = client.chat.completions.create(\n",
        "            model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Summaries only.\"},\n",
        "                {\"role\": \"user\", \"content\": compression_prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            stream=False\n",
        "        )\n",
        "\n",
        "        summary = comp_resp.choices[0].message.content.strip()\n",
        "        buffer = summary\n",
        "\n",
        "        if len(buffer) >= max_chars:\n",
        "            break\n",
        "\n",
        "    final_prompt = (\n",
        "        \"Here is all the gathered context. Analyze the meme thoroughly.\\n\\n\"\n",
        "        f\"{buffer[:max_chars]}\"\n",
        "    )\n",
        "\n",
        "    final_resp = client.chat.completions.create(\n",
        "        model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Precise analysis.\"},\n",
        "            {\"role\": \"user\", \"content\": final_prompt}\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    print(final_prompt)\n",
        "    output = final_resp.choices[0].message.content\n",
        "\n",
        "    # safe append\n",
        "    with conv_lock:\n",
        "        conversation += \"This is the intial input: \" + final_prompt + \"\\n\"\n",
        "        conversation += \"This is the intial output: \" + output + \"\\n\"\n",
        "\n",
        "    return jsonify({\"input\": user_input, \"output\": output})\n",
        "\n",
        "\n",
        "@app.route(\"/api/converse\", methods=[\"POST\"])\n",
        "def converse():\n",
        "    global conversation\n",
        "    data = request.json\n",
        "    user_input = data.get(\"input\", \"\")\n",
        "\n",
        "    with conv_lock:\n",
        "        prompt_context = conversation\n",
        "        conversation += \"This is added inputs by the user: \" + user_input + \"\\n\"\n",
        "\n",
        "    prompt = (\n",
        "        \"Hopefully, make a 2-3 consentences. \"\n",
        "        \"This is the most recent input (Focus on this): \" + user_input + \"\\n\"\n",
        "        f\"CONTEXT:\\n{prompt_context}\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Be precise. Output JSON only.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "        messages=messages,\n",
        "        temperature=0.4,\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    output = resp.choices[0].message.content\n",
        "\n",
        "    with conv_lock:\n",
        "        conversation += \"This is the added output: \" + output + \"\\n\"\n",
        "\n",
        "    return jsonify({\"input\": user_input, \"output\": output})\n",
        "\n",
        "\n",
        "@app.route(\"/api/search_image\", methods=[\"POST\"])\n",
        "def search_image():\n",
        "    data = request.json\n",
        "    query = data.get(\"query\", \"\")\n",
        "\n",
        "    if not query:\n",
        "        return jsonify({\"error\": \"query field required\"}), 400\n",
        "\n",
        "    img_url = kym_search_image(query)\n",
        "    if img_url is None:\n",
        "        return jsonify({\"query\": query, \"image_url\": None, \"status\": \"not_found\"})\n",
        "\n",
        "    return jsonify({\"query\": query, \"image_url\": img_url})\n",
        "\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"34oJGiQHYcc8THKsPzsqUXBLMi6_5oBmkk9dHcsN2T2S5W2gG\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = ngrok.connect(5001).public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "app.run(port=5001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKoZPprxio3H",
        "outputId": "c1f32426-aba6-4e4d-c3fd-e1ce000bf60e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://nonrealistic-ungrimed-luvenia.ngrok-free.dev\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5001\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Nov/2025 12:16:20] \"OPTIONS /api/respond HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: https://knowyourmeme.com/memes/my-mom-is-kinda-homeless-please-speed-i-need-this\n",
            "Image: https://i.kym-cdn.com/entries/icons/original/000/055/583/my-mom-is-kinda-homless.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Nov/2025 12:17:09] \"POST /api/respond HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is all the gathered context. Analyze the meme thoroughly.\n",
            "\n",
            "About: My Mom is Kinda Homeless and Please Speed I Need This , also known as My Mom is Kind of Homeless , refer to quotes from the IShowSpeed Trying Not To Laugh clip, said by an emotional kid trying to get streamer IShowSpeed (or Speed) to throw a Fortnite game in his \" EARLY STREAM! \" livestream from late December 2021. Speed is seen trying not to laugh at the kid's pleading, and is then seen smiling when another player calls him out, saying, \"Speed, I'm watching your stream, why are you trying not to laugh, bruh?\" The clip went notably viral when it was used as a reaction GIF and in video caption memes in 2023. The kid's quote about his mom being kind of homeless gained its own memetic spread as a catchphrase and phrasal template in the following years.\n",
            "In late October 2025, IShowSpeed recreated the My Mom is Kinda Homeless clip in a \"reunion\" stream with the two guys in the original clip. The kid who said, \"My mom is kinda homeless,\" was revealed to be named Acey, and he and his mom confirmed that she was kind of homeless at the time. The other kid was revealed to be named Pew. During the stream, Speed also stated that he would buy the mom a house in Las Vegas.\n",
            "Origin: On December 29th, 2021, streamer IShowSpeed went live on YouTube  for roughly four and a half hours. He played Fortnite and Five Nights at Freddy's and was seen wearing an Adidas sweatshirt and a black shiesty. The stream was titled \"EARLY STREAM!\" and it received over 6.9 million views in four years.\n",
            "The stream was the source of the IShowSpeed Smiling Trying Not To Laugh meme, which used a clip from the stream's 2:08:05 timestamp, when a younger player pleads with Speed to throw the game so that he can win, saying, \"Please, Speed, I need this. My mom is kind of homeless. I live with my dad. I wanna help her out.\"\n",
            "Spread: The clip of IShowSpeed suppressing his laugh came to be used as a reaction GIF and video in 2023, with the earliest known viral post using the clip being shared on October 24th, 2023. It was shared by Twitter / X  user @VenomDbd, who captioned it, \"Peter when he saw the 'Miles Morales Original' Suit,\" gaining over 21,000 likes in two years.\n",
            "On October 26th, TikToker  @timmy_burch used the clip in a POV video caption meme , reading, \"POV: me tryna hold in my laugh during a serious conversation.\" Over two years, the video received roughly 30,400 likes.\n",
            "On April 24th, 2024, X  user @yandhiisntreal tweeted, \"Please speed, I NEED THISðŸ˜­ðŸ˜­ðŸ˜­ my mom is kinda homelessðŸ˜¢ðŸ˜¢ðŸ˜¢,\" gaining over 27,000 likes in a year.\n",
            "On June 10th, X  user @XyloBayo posted a redraw of IShowSpeed Trying Not to Laugh, showing Mob Psycho 100 character Arataka Reigen. The redraw was captioned, \"My mom is kinda homeless,\" gaining over 16,000 likes in a year.\n",
            "On September 17th, 2024, X  user @Real_BryanPlush tweeted a redraw showing Splatoon character Shiver Hohojiro, paired with a caption that used the My Mom is Kinda Homeless phrasal template, reading, \"Please Shiver, I NEED THIS! My Mom Is Kinda Homeless.\" Over one year, the post gained roughly 9,600 likes.\n",
            "Memes and viral posts using the quote continued in 2025. For instance, on February 17th, Redditor Professional-Swim-85 shared a post to the /r/Genshin_Impact  subreddit, showing Genshin Impact character Mizuki as Speed in a GIF format, captioned, \"'My mom is kinda homelessâ€¦'\" Over eight months, the post received over 4,500 upvotes.\n",
            "On October 26th, 2025, IShowSpeed held a reunion stream on YouTube  with the two guys from the original My Mom is Kinda Homeless clip, receiving over 5.6 million views in three days.\n",
            "At the stream's 18:12 timestamp,  Speed invited Acey onto the stream, who was the kid who said, \"My mom is kinda homeless,\" in the original video. Acey said that he was 12 years old in the original. Later, Speed talked to Acey's mom, who confirmed that she was kind of homeless back in December 2021.\n",
            "At the 19:03 timestamp,  Speed invited Pew onto the stream, who was the other kid who said, \"Why are you trying not to laugh, bruh?\" in the original clip. Notably, when Pew was added to a video call with Speed later in the stream, he put a fake gun in his mouth.\n",
            "Later on October 26th, X  user @FearedBuck shared a clip from the stream, showing Speed recreating the My Mom is Kinda Homeless clip live with the two guys, gaining over 55,000 likes in three days.\n",
            "On that same day, X  user @scubaryan_ shared a side-by-side comparison between the original clip and the recreation, gaining over 16,000 likes in three days.\n",
            "External References:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Nov/2025 12:18:23] \"OPTIONS /api/respond HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: https://knowyourmeme.com/memes/my-mom-is-kinda-homeless-please-speed-i-need-this\n",
            "Image: https://i.kym-cdn.com/entries/icons/original/000/055/583/my-mom-is-kinda-homless.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Nov/2025 12:19:05] \"POST /api/respond HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is all the gathered context. Analyze the meme thoroughly.\n",
            "\n",
            "About: My Mom is Kinda Homeless and Please Speed I Need This , also known as My Mom is Kind of Homeless , refer to quotes from the IShowSpeed Trying Not To Laugh clip, said by an emotional kid trying to get streamer IShowSpeed (or Speed) to throw a Fortnite game in his \" EARLY STREAM! \" livestream from late December 2021. Speed is seen trying not to laugh at the kid's pleading, and is then seen smiling when another player calls him out, saying, \"Speed, I'm watching your stream, why are you trying not to laugh, bruh?\" The clip went notably viral when it was used as a reaction GIF and in video caption memes in 2023. The kid's quote about his mom being kind of homeless gained its own memetic spread as a catchphrase and phrasal template in the following years.\n",
            "In late October 2025, IShowSpeed recreated the My Mom is Kinda Homeless clip in a \"reunion\" stream with the two guys in the original clip. The kid who said, \"My mom is kinda homeless,\" was revealed to be named Acey, and he and his mom confirmed that she was kind of homeless at the time. The other kid was revealed to be named Pew. During the stream, Speed also stated that he would buy the mom a house in Las Vegas.\n",
            "Origin: On December 29th, 2021, streamer IShowSpeed went live on YouTube  for roughly four and a half hours. He played Fortnite and Five Nights at Freddy's and was seen wearing an Adidas sweatshirt and a black shiesty. The stream was titled \"EARLY STREAM!\" and it received over 6.9 million views in four years.\n",
            "The stream was the source of the IShowSpeed Smiling Trying Not To Laugh meme, which used a clip from the stream's 2:08:05 timestamp, when a younger player pleads with Speed to throw the game so that he can win, saying, \"Please, Speed, I need this. My mom is kind of homeless. I live with my dad. I wanna help her out.\"\n",
            "Spread: The clip of IShowSpeed suppressing his laugh came to be used as a reaction GIF and video in 2023, with the earliest known viral post using the clip being shared on October 24th, 2023. It was shared by Twitter / X  user @VenomDbd, who captioned it, \"Peter when he saw the 'Miles Morales Original' Suit,\" gaining over 21,000 likes in two years.\n",
            "On October 26th, TikToker  @timmy_burch used the clip in a POV video caption meme , reading, \"POV: me tryna hold in my laugh during a serious conversation.\" Over two years, the video received roughly 30,400 likes.\n",
            "On April 24th, 2024, X  user @yandhiisntreal tweeted, \"Please speed, I NEED THISðŸ˜­ðŸ˜­ðŸ˜­ my mom is kinda homelessðŸ˜¢ðŸ˜¢ðŸ˜¢,\" gaining over 27,000 likes in a year.\n",
            "On June 10th, X  user @XyloBayo posted a redraw of IShowSpeed Trying Not to Laugh, showing Mob Psycho 100 character Arataka Reigen. The redraw was captioned, \"My mom is kinda homeless,\" gaining over 16,000 likes in a year.\n",
            "On September 17th, 2024, X  user @Real_BryanPlush tweeted a redraw showing Splatoon character Shiver Hohojiro, paired with a caption that used the My Mom is Kinda Homeless phrasal template, reading, \"Please Shiver, I NEED THIS! My Mom Is Kinda Homeless.\" Over one year, the post gained roughly 9,600 likes.\n",
            "Memes and viral posts using the quote continued in 2025. For instance, on February 17th, Redditor Professional-Swim-85 shared a post to the /r/Genshin_Impact  subreddit, showing Genshin Impact character Mizuki as Speed in a GIF format, captioned, \"'My mom is kinda homelessâ€¦'\" Over eight months, the post received over 4,500 upvotes.\n",
            "On October 26th, 2025, IShowSpeed held a reunion stream on YouTube  with the two guys from the original My Mom is Kinda Homeless clip, receiving over 5.6 million views in three days.\n",
            "At the stream's 18:12 timestamp,  Speed invited Acey onto the stream, who was the kid who said, \"My mom is kinda homeless,\" in the original video. Acey said that he was 12 years old in the original. Later, Speed talked to Acey's mom, who confirmed that she was kind of homeless back in December 2021.\n",
            "At the 19:03 timestamp,  Speed invited Pew onto the stream, who was the other kid who said, \"Why are you trying not to laugh, bruh?\" in the original clip. Notably, when Pew was added to a video call with Speed later in the stream, he put a fake gun in his mouth.\n",
            "Later on October 26th, X  user @FearedBuck shared a clip from the stream, showing Speed recreating the My Mom is Kinda Homeless clip live with the two guys, gaining over 55,000 likes in three days.\n",
            "On that same day, X  user @scubaryan_ shared a side-by-side comparison between the original clip and the recreation, gaining over 16,000 likes in three days.\n",
            "External References:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Nov/2025 12:19:28] \"OPTIONS /api/converse HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Nov/2025 12:19:34] \"POST /api/converse HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "sqRVnsZHBP9q"
      },
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "conversation = \"\"\n",
        "client = OpenAI(base_url=\"http://127.0.0.1:5000/v1\", api_key=\"dummy\")\n",
        "\n",
        "@app.route(\"/api/respond\", methods=[\"POST\"])\n",
        "def respond():\n",
        "    data = request.json\n",
        "    user_input = data.get(\"input\", \"\")\n",
        "\n",
        "    # initial context from KYM\n",
        "    page_url, img_url = kym_search_image(user_input)\n",
        "    sections = scrape_text_kym(page_url)\n",
        "\n",
        "    context = \"\"\n",
        "    for title, text in sections.items():\n",
        "        context += f\"{title}: {text}\\n\"\n",
        "\n",
        "    # running text buffer\n",
        "    buffer = context.strip()\n",
        "    max_chars = 2500 + len(context)\n",
        "\n",
        "    # loop until buffer fills\n",
        "    while len(buffer) < max_chars:\n",
        "        # ask model for new search terms\n",
        "        prompt = (\n",
        "            \"Given the following context, propose 3â€“5 broad google search terms \"\n",
        "            \"that would expand the context. Return ONLY a JSON array of strings.\\n\\n\"\n",
        "            f\"CONTEXT:\\n{buffer[:2000]}\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Be precise. Output JSON only.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "            messages=messages,\n",
        "            temperature=0.4,\n",
        "            stream=False\n",
        "        )\n",
        "\n",
        "        raw_terms = resp.choices[0].message.content\n",
        "        try:\n",
        "            import json\n",
        "            terms = json.loads(raw_terms)\n",
        "            if not isinstance(terms, list):\n",
        "                break\n",
        "        except:\n",
        "            break\n",
        "\n",
        "        # scrape google summaries for each term\n",
        "        for term in terms:\n",
        "            google_page = f\"https://www.google.com/search?q={term.replace(' ', '+')}\"\n",
        "            scraped = scrape_visible_text(google_page)\n",
        "            if scraped:\n",
        "                buffer += \"\\n\" + scraped\n",
        "                if len(buffer) >= max_chars:\n",
        "                    break\n",
        "\n",
        "        # compress via model\n",
        "        compression_prompt = (\n",
        "            \"Summarize the important points of the following text. \"\n",
        "            \"Be succinct and do not exceed 2 paragraphs.\\n\\n\"\n",
        "            f\"{buffer[-3000:]}\"\n",
        "        )\n",
        "        comp_resp = client.chat.completions.create(\n",
        "            model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Summaries only.\"},\n",
        "                {\"role\": \"user\", \"content\": compression_prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            stream=False\n",
        "        )\n",
        "\n",
        "        summary = comp_resp.choices[0].message.content.strip()\n",
        "        buffer = summary  # next loop uses summary as new context\n",
        "\n",
        "        if len(buffer) >= max_chars:\n",
        "            break\n",
        "\n",
        "    # final answer request\n",
        "    final_prompt = (\n",
        "        \"Here is all the gathered context. Analyze the meme thoroughly.\\n\\n\"\n",
        "        f\"{buffer[:max_chars]}\"\n",
        "    )\n",
        "\n",
        "    final_resp = client.chat.completions.create(\n",
        "        model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Precise analysis.\"},\n",
        "            {\"role\": \"user\", \"content\": final_prompt}\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "        stream=False\n",
        "    )\n",
        "    print(final_prompt)\n",
        "\n",
        "    conversation += \"This is the intial input: \" + final_prompt + \"\\n\";\n",
        "    output = final_resp.choices[0].message.content\n",
        "    conversation += \"This is the intial output: \" + output + \"\\n\";\n",
        "\n",
        "    return jsonify({\"input\": user_input, \"output\": output})\n",
        "\n",
        "@app.route(\"/api/converse\", methods=[\"POST\"])\n",
        "def converse():\n",
        "  data = request.json\n",
        "  user_input = data.get(\"input\", \"\")\n",
        "  prompt = (\n",
        "    \"Given the following context, give a response to the most recent input. Hopefully, 2-3 sentences. \"\n",
        "    \"This is the most recent input (Focus on this): \" + user_input + \"\\n\"\n",
        "    f\"CONTEXT:\\n{conversation}\"\n",
        "        )\n",
        "\n",
        "  conversation += \"This is added inputs by the user: \" + user_input + \"\\n\";\n",
        "\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"Be precise. Output JSON only.\"},\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "  resp = client.chat.completions.create(\n",
        "      model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "      messages=messages,\n",
        "      temperature=0.4,\n",
        "      stream=False\n",
        "  )\n",
        "\n",
        "  output = resp.choices[0].message.content\n",
        "\n",
        "  conversation += \"This is the added output: \" + output + \"\\n\";\n",
        "\n",
        "  return jsonify({\"input\": user_input, \"output\": output})\n",
        "\n",
        "@app.route(\"/api/search_image\", methods=[\"POST\"])\n",
        "def search_image():\n",
        "    data = request.json\n",
        "    query = data.get(\"query\", \"\")\n",
        "\n",
        "    if not query:\n",
        "        return jsonify({\"error\": \"query field required\"}), 400\n",
        "\n",
        "    img_url = kym_search_image(query)\n",
        "\n",
        "    if img_url is None:\n",
        "        return jsonify({\"query\": query, \"image_url\": None, \"status\": \"not_found\"})\n",
        "\n",
        "    return jsonify({\"query\": query, \"image_url\": img_url})\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"34oJGiQHYcc8THKsPzsqUXBLMi6_5oBmkk9dHcsN2T2S5W2gG\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = ngrok.connect(5001).public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# -------------------\n",
        "# Run server\n",
        "# -------------------\n",
        "app.run(port=5001)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}