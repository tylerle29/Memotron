{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylerle29/Memotron/blob/main/MEMoTron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blQrFsAXcPbn",
        "outputId": "a55dfc3c-8d55-4840-f5a2-6c1291180739"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Llama-3.1-Nemotron-Nano-4B-v1.1'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 85 (delta 7), reused 0 (delta 0), pack-reused 70 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (85/85), 84.87 KiB | 2.83 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.42 GiB | 97.42 MiB/s, done.\n",
            "Encountered 2 file(s) that may not have been copied correctly on Windows:\n",
            "\tmodel-00002-of-00002.safetensors\n",
            "\tmodel-00001-of-00002.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU vllm"
      ],
      "metadata": {
        "id": "ecUhxn_lcrXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d613f97-ee7d-46a8-8a24-116f1fbab2c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.2/438.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.0/180.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m138.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.7/285.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup python -m vllm.entrypoints.openai.api_server \\\n",
        "  --model Llama-3.1-Nemotron-Nano-4B-v1.1 \\\n",
        "  --trust-remote-code \\\n",
        "  --seed 1 \\\n",
        "  --host \"0.0.0.0\" \\\n",
        "  --port 5000 \\\n",
        "  --served-model-name \"Llama-Nemotron-Nano-4B-v1.1\" \\\n",
        "  --tensor-parallel-size 1 \\\n",
        "  --max-model-len 50_000 \\\n",
        "  --gpu-memory-utilization 0.95 \\\n",
        "  --enforce-eager \\\n",
        "  --enable-auto-tool-choice \\\n",
        "  --tool-parser-plugin \"Llama-3.1-Nemotron-Nano-4B-v1.1/llama_nemotron_nano_toolcall_parser.py\" \\\n",
        "  --tool-call-parser \"llama_nemotron_json\" \\\n",
        "  --chat-template \"Llama-3.1-Nemotron-Nano-4B-v1.1/llama_nemotron_nano_generic_tool_calling.jinja\" \\\n",
        "  > vllm.log &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMVwZCTZdsSG",
        "outputId": "6f5b1b34-9585-48b0-ff71-4a55270d1afe"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!while ! grep -q \"Application startup complete\" vllm.log; do tail -n 1 vllm.log; sleep 5; done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnaEKAPTdtIc",
        "outputId": "68090b9b-0bb9-4617-82d4-8e3ca0ee998b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n",
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n",
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n",
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n",
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n",
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n",
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n",
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n",
            "\u001b[1;36m(APIServer pid=30324)\u001b[0;0m INFO:     Application shutdown complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors\n",
        "!pip install flask pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2vCVzRvt2I5",
        "outputId": "069f1347-9ba8-4bd6-fa95-ed8cbaaf94bf"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.12/dist-packages (6.0.1)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.12/dist-packages (from flask-cors) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.12/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=0.9->flask-cors) (3.0.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (3.1.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import quote\n",
        "import re\n",
        "\n",
        "def kym_search_image(query):\n",
        "    url = f\"https://knowyourmeme.com/search?q={quote(query)}\"\n",
        "    r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "    # 1. find the first meme-page link that contains an image with data-image\n",
        "    candidates = soup.select('a[href^=\"/memes/\"]')\n",
        "    page = None\n",
        "\n",
        "    for a in candidates:\n",
        "        img = a.select_one(\"img[data-image]\")\n",
        "        if img:\n",
        "            page = a\n",
        "            img_tag = img\n",
        "            break\n",
        "\n",
        "    if page is None:\n",
        "        print(\"No match\")\n",
        "        return None\n",
        "\n",
        "    # page URL\n",
        "    page_url = \"https://knowyourmeme.com\" + page[\"href\"]\n",
        "\n",
        "    # high-quality image from data-image\n",
        "    img_url = img_tag[\"data-image\"]\n",
        "\n",
        "    print(\"Page:\", page_url)\n",
        "    print(\"Image:\", img_url)\n",
        "    return (page_url, img_url)\n",
        "\n",
        "def scrape_text_kym(url):\n",
        "\n",
        "    # 1. Request webpage\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    response.raise_for_status()\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Parse HTML\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Sets up the sections we need to parse.\n",
        "\n",
        "    sections = {}\n",
        "\n",
        "    relevant_titles = [\n",
        "\n",
        "    \"About\", \"Origin\", \"Spread\", \"History\", \"Background\", \"Meaning\", \"Usage\", \"Development\", \"Overview\", \"External References\"\n",
        "\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    # Parses them.\n",
        "\n",
        "    for h2 in soup.find_all(\"h2\"):\n",
        "\n",
        "        title = h2.get_text(strip=True)\n",
        "\n",
        "        if title in relevant_titles:\n",
        "\n",
        "            content = []\n",
        "\n",
        "\n",
        "\n",
        "            # Walk through siblings until the next h2\n",
        "\n",
        "            for sib in h2.next_siblings:\n",
        "\n",
        "                if getattr(sib, \"name\", None) == \"h2\":\n",
        "\n",
        "                    break\n",
        "\n",
        "                if sib.name in [\"p\"]:\n",
        "\n",
        "                    text = sib.get_text(\" \", strip=True)\n",
        "\n",
        "                    if text:\n",
        "                        text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
        "                        content.append(text.strip())\n",
        "\n",
        "\n",
        "\n",
        "            sections[title] = \"\\n\".join(content)\n",
        "\n",
        "\n",
        "\n",
        "    return sections\n",
        "\n",
        "def scrape_visible_text(url, timeout=10):\n",
        "    r = requests.get(url, timeout=timeout, headers={\n",
        "        \"User-Agent\": \"Mozilla/5.0\"\n",
        "    })\n",
        "    r.raise_for_status()\n",
        "    html = r.text\n",
        "    # remove all HTML tags\n",
        "    text = re.sub(r\"<[^>]+>\", \" \", html)\n",
        "    # collapse whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def scrape_all_paragraphs(url):\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "  response = requests.get(url, headers=headers)\n",
        "  response.raise_for_status()\n",
        "\n",
        "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "  paragraphs = []\n",
        "  for p in soup.find_all(\"p\"):\n",
        "      text = p.get_text(\" \", strip=True)\n",
        "      if text:\n",
        "          paragraphs.append(text)\n",
        "\n",
        "  return \"\\n\".join(paragraphs)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "S6kyQizhxu95"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "coversation = \"\"\n",
        "client = OpenAI(base_url=\"http://127.0.0.1:5000/v1\", api_key=\"dummy\")\n",
        "\n",
        "@app.route(\"/api/respond\", methods=[\"POST\"])\n",
        "def respond():\n",
        "    data = request.json\n",
        "    user_input = data.get(\"input\", \"\")\n",
        "\n",
        "    # initial context from KYM\n",
        "    page_url, img_url = kym_search_image(user_input)\n",
        "    sections = scrape_text_kym(page_url)\n",
        "\n",
        "    context = \"\"\n",
        "    for title, text in sections.items():\n",
        "        context += f\"{title}: {text}\\n\"\n",
        "\n",
        "    # running text buffer\n",
        "    combined = \"\"   # unlimited KYM text\n",
        "    agentic = \"\"             # only new scraped material\n",
        "    agentic_limit = 2500\n",
        "\n",
        "\n",
        "    # loop until buffer fills\n",
        "    while len(agentic) < agentic_limit:\n",
        "      combined = context + \"\\n\" + agentic\n",
        "\n",
        "      # 1. ask model for search terms\n",
        "      prompt = (\n",
        "          \"Given the following context, propose 3–5 broad search terms. \"\n",
        "          \"Return ONLY a JSON array.\\n\\n\"\n",
        "          f\"{combined}\"\n",
        "      )\n",
        "\n",
        "      resp = client.chat.completions.create(\n",
        "          model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": \"JSON only.\"},\n",
        "              {\"role\": \"user\", \"content\": prompt}\n",
        "          ],\n",
        "          temperature=0.4,\n",
        "          stream=False\n",
        "      )\n",
        "\n",
        "      import json\n",
        "      try:\n",
        "          terms = json.loads(resp.choices[0].message.content)\n",
        "          if not isinstance(terms, list):\n",
        "              break\n",
        "      except:\n",
        "          break\n",
        "\n",
        "      # 2. scrape Google summaries for each term\n",
        "      for term in terms:\n",
        "          url = \"https://www.google.com/search?q=\" + term.replace(\" \", \"+\")\n",
        "          scraped = scrape_visible_text(url)\n",
        "          if scraped:\n",
        "              agentic += \"\\n\" + scraped\n",
        "\n",
        "          if len(agentic) >= agentic_limit:\n",
        "              break\n",
        "\n",
        "      # 3. compress ONLY the agentic portion\n",
        "      compression_prompt = (\n",
        "          \"Summarize the following text. Maximum length 2500 characters.\\n\\n\"\n",
        "          f\"{agentic[-3000:]}\"\n",
        "      )\n",
        "\n",
        "      comp = client.chat.completions.create(\n",
        "          model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "          messages=[\n",
        "              {\"role\": \"system\", \"content\": \"Summaries only.\"},\n",
        "              {\"role\": \"user\", \"content\": compression_prompt}\n",
        "          ],\n",
        "          temperature=0.2,\n",
        "          stream=False\n",
        "      )\n",
        "\n",
        "      agentic = comp.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "    # final answer request\n",
        "    final_prompt = (\n",
        "        \"Here is all the gathered context. Analyze the meme thoroughly.\\n\\n\"\n",
        "        f\"{combined}\"\n",
        "    )\n",
        "    print(final_prompt)\n",
        "    final_resp = client.chat.completions.create(\n",
        "        model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Precise analysis.\"},\n",
        "            {\"role\": \"user\", \"content\": final_prompt}\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "        stream=False\n",
        "    )\n",
        "\n",
        "    conversation += \"This is the intial input: \" + final_prompt + \"\\n\";\n",
        "    output = final_resp.choices[0].message.content\n",
        "    conversation += \"This is the intial output: \" + output + \"\\n\";\n",
        "    return jsonify({\"input\": user_input, \"output\": output})\n",
        "\n",
        "@app.route(\"/api/converse\", methods=[\"POST\"])\n",
        "def converse():\n",
        "  data = request.json\n",
        "  user_input = data.get(\"input\", \"\")\n",
        "  prompt = (\n",
        "    \"Given the following context, give a response to the most recent input. Hopefully, 2-3 sentences. \"\n",
        "    \"This is the most recent input (Focus on this): \" + user_input + \"\\n\"\n",
        "    f\"CONTEXT:\\n{coversation}\"\n",
        "        )\n",
        "\n",
        "  coversation += \"This is added inputs by the user: \" + user_input + \"\\n\";\n",
        "\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"Be precise. Output JSON only.\"},\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "  resp = client.chat.completions.create(\n",
        "      model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "      messages=messages,\n",
        "      temperature=0.4,\n",
        "      stream=False\n",
        "  )\n",
        "\n",
        "  output = resp.choices[0].message.content\n",
        "\n",
        "  conversation += \"This is the added output: \" + output + \"\\n\";\n",
        "\n",
        "  return jsonify({\"input\": user_input, \"output\": output})\n",
        "\n",
        "@app.route(\"/api/search_image\", methods=[\"POST\"])\n",
        "def search_image():\n",
        "    data = request.json\n",
        "    query = data.get(\"query\", \"\")\n",
        "\n",
        "    if not query:\n",
        "        return jsonify({\"error\": \"query field required\"}), 400\n",
        "\n",
        "    img_url = kym_search_image(query)\n",
        "\n",
        "    if img_url is None:\n",
        "        return jsonify({\"query\": query, \"image_url\": None, \"status\": \"not_found\"})\n",
        "\n",
        "    return jsonify({\"query\": query, \"image_url\": img_url})\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"34oJGiQHYcc8THKsPzsqUXBLMi6_5oBmkk9dHcsN2T2S5W2gG\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = ngrok.connect(5001).public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# -------------------\n",
        "# Run server\n",
        "# -------------------\n",
        "app.run(port=5001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLCMK8EZLoLp",
        "outputId": "fb266bd0-05e7-4bd1-9c37-e03279b72eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://nonrealistic-ungrimed-luvenia.ngrok-free.dev\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5001\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69c3655-4a74-45f7-fc3c-1e78d6adca61",
        "collapsed": true,
        "id": "sqRVnsZHBP9q"
      },
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "coversation = \"\"\n",
        "client = OpenAI(base_url=\"http://127.0.0.1:5000/v1\", api_key=\"dummy\")\n",
        "\n",
        "@app.route(\"/api/respond\", methods=[\"POST\"])\n",
        "def respond():\n",
        "    data = request.json\n",
        "    user_input = data.get(\"input\", \"\")\n",
        "\n",
        "    # initial context from KYM\n",
        "    page_url, img_url = kym_search_image(user_input)\n",
        "    sections = scrape_text_kym(page_url)\n",
        "\n",
        "    context = \"\"\n",
        "    for title, text in sections.items():\n",
        "        context += f\"{title}: {text}\\n\"\n",
        "\n",
        "    # running text buffer\n",
        "    buffer = context.strip()\n",
        "    max_chars = 2500 + len(context)\n",
        "\n",
        "    # loop until buffer fills\n",
        "    while len(buffer) < max_chars:\n",
        "        # ask model for new search terms\n",
        "        prompt = (\n",
        "            \"Given the following context, propose 3–5 broad google search terms \"\n",
        "            \"that would expand the context. Return ONLY a JSON array of strings.\\n\\n\"\n",
        "            f\"CONTEXT:\\n{buffer[:2000]}\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"Be precise. Output JSON only.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "            messages=messages,\n",
        "            temperature=0.4,\n",
        "            stream=False\n",
        "        )\n",
        "\n",
        "        raw_terms = resp.choices[0].message.content\n",
        "        try:\n",
        "            import json\n",
        "            terms = json.loads(raw_terms)\n",
        "            if not isinstance(terms, list):\n",
        "                break\n",
        "        except:\n",
        "            break\n",
        "\n",
        "        # scrape google summaries for each term\n",
        "        for term in terms:\n",
        "            google_page = f\"https://www.google.com/search?q={term.replace(' ', '+')}\"\n",
        "            scraped = scrape_visible_text(google_page)\n",
        "            if scraped:\n",
        "                buffer += \"\\n\" + scraped\n",
        "                if len(buffer) >= max_chars:\n",
        "                    break\n",
        "\n",
        "        # compress via model\n",
        "        compression_prompt = (\n",
        "            \"Summarize the important points of the following text. \"\n",
        "            \"Be succinct and do not exceed 2 paragraphs.\\n\\n\"\n",
        "            f\"{buffer[-3000:]}\"\n",
        "        )\n",
        "        comp_resp = client.chat.completions.create(\n",
        "            model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Summaries only.\"},\n",
        "                {\"role\": \"user\", \"content\": compression_prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            stream=False\n",
        "        )\n",
        "\n",
        "        summary = comp_resp.choices[0].message.content.strip()\n",
        "        buffer = summary  # next loop uses summary as new context\n",
        "\n",
        "        if len(buffer) >= max_chars:\n",
        "            break\n",
        "\n",
        "    # final answer request\n",
        "    final_prompt = (\n",
        "        \"Here is all the gathered context. Analyze the meme thoroughly.\\n\\n\"\n",
        "        f\"{buffer[:max_chars]}\"\n",
        "    )\n",
        "\n",
        "    final_resp = client.chat.completions.create(\n",
        "        model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Precise analysis.\"},\n",
        "            {\"role\": \"user\", \"content\": final_prompt}\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "        stream=False\n",
        "    )\n",
        "    print(final_prompt)\n",
        "\n",
        "    conversation += \"This is the intial input: \" + final_prompt + \"\\n\";\n",
        "    output = final_resp.choices[0].message.content\n",
        "    conversation += \"This is the intial output: \" + output + \"\\n\";\n",
        "\n",
        "    return jsonify({\"input\": user_input, \"output\": output})\n",
        "\n",
        "@app.route(\"/api/converse\", methods=[\"POST\"])\n",
        "def converse():\n",
        "  data = request.json\n",
        "  user_input = data.get(\"input\", \"\")\n",
        "  prompt = (\n",
        "    \"Given the following context, give a response to the most recent input. Hopefully, 2-3 sentences. \"\n",
        "    \"This is the most recent input (Focus on this): \" + user_input + \"\\n\"\n",
        "    f\"CONTEXT:\\n{coversation}\"\n",
        "        )\n",
        "\n",
        "  coversation += \"This is added inputs by the user: \" + user_input + \"\\n\";\n",
        "\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"Be precise. Output JSON only.\"},\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        "\n",
        "  resp = client.chat.completions.create(\n",
        "      model=\"Llama-Nemotron-Nano-4B-v1.1\",\n",
        "      messages=messages,\n",
        "      temperature=0.4,\n",
        "      stream=False\n",
        "  )\n",
        "\n",
        "  output = resp.choices[0].message.content\n",
        "\n",
        "  conversation += \"This is the added output: \" + output + \"\\n\";\n",
        "\n",
        "  return jsonify({\"input\": user_input, \"output\": output})\n",
        "\n",
        "@app.route(\"/api/search_image\", methods=[\"POST\"])\n",
        "def search_image():\n",
        "    data = request.json\n",
        "    query = data.get(\"query\", \"\")\n",
        "\n",
        "    if not query:\n",
        "        return jsonify({\"error\": \"query field required\"}), 400\n",
        "\n",
        "    img_url = kym_search_image(query)\n",
        "\n",
        "    if img_url is None:\n",
        "        return jsonify({\"query\": query, \"image_url\": None, \"status\": \"not_found\"})\n",
        "\n",
        "    return jsonify({\"query\": query, \"image_url\": img_url})\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"34oJGiQHYcc8THKsPzsqUXBLMi6_5oBmkk9dHcsN2T2S5W2gG\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = ngrok.connect(5001).public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# -------------------\n",
        "# Run server\n",
        "# -------------------\n",
        "app.run(port=5001)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://nonrealistic-ungrimed-luvenia.ngrok-free.dev\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5001\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [09/Nov/2025 10:27:09] \"OPTIONS /api/respond HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page: https://knowyourmeme.com/memes/events/donald-trumps-grocery-id-gaffe\n",
            "Image: https://i.kym-cdn.com/entries/icons/original/000/026/822/trump.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/Nov/2025 10:27:51] \"POST /api/respond HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is all the gathered context. Analyze the meme thoroughly.\n",
            "\n",
            "Overview: Donald Trump's Grocery ID Gaffe refers to the reaction to a speech made by United States President Donald Trump , during which the president erroneously claimed that in order to purchase groceries, customers must have identification.\n",
            "Background: On July 31st, 2018, at a rally in Tampa, Florida, President Trump delivered remarks in support of voter identification laws. During his speech, he compared voter ID laws to buying groceries, which he claimed people need identification to purchase. He said:\n",
            "Development: Reactions to Trump's comments were split between people discussing their issues with voter ID laws and people mocking Trump for his assertion that people need identification to purchase groceries. Twitter  user @stuartpstevens tweeted, \"We can’t get half of eligible American citizens to vote. Illegal voting is a felony and the idea that lots of people would be so hot to vote they’d commit a felony is one of the most illogical conspiracies imaginable.\" Within 24 hours, the post (shown below, left) received more than 500 retweets and 1,700 likes.\n",
            "Twitter  user @JasonKander tweeted, \"The president says you need a photo ID to buy groceries. 1. He’s lying. 2. I didn’t go to Afghanistan for his right to buy groceries. 3. This dude has never in his life bought his own groceries.\" The post (shown below, center) received more than 9,400 retweets and 48,000 likes in 24 hours.\n",
            "People continued to share jokes about the gaffe by using the hashtag \"#groceryID. Twitter  @cherryswirlgirl tweeted a GIF of the character Soup Nazi from the television series Seinfeld and the caption \"No ID? Then no soup for you! #groceryid.\" The post (shown below, right) received more than 15 retweet sand 90 likes in 24 hours.\n",
            "Twitter user @DevonInSpace tweeted a GIF from the telelvision series Arrested Development of the character Lucille Bluth saying \"I mean it's one banana, Michael, What could it cost? 10 dollars?\" They captioned the tweet, \"When Trump said you need ID to buy groceries. #TrumpTampa. The post (shown below) received more than 1,000 retweets and 3,000 likes in 24 hours.\n",
            "That evening, Twitter  published a Moments page about the reaction to the speech.\n",
            "Several media outlets covered the gaffe, including Heavy,  Business Insider,  TIME,  The Takeout,  The Daily Dot  and more.\n",
            "External References:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7P6fFFqrDYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "# Delete variables that might be holding onto GPU memory\n",
        "# You might need to add other variable names here if you have more objects on the GPU\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if 'pipe' in locals():\n",
        "    del pipe\n",
        "if 'pipe_smaller' in locals():\n",
        "    del pipe_smaller\n",
        "if 'input_ids' in locals():\n",
        "    del input_ids\n",
        "if 'outputs' in locals():\n",
        "    del outputs\n",
        "if 'tokenizer' in locals():\n",
        "    del tokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\n",
        "\n",
        "# Load model (on GPU if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
        "    torch_dtype=torch.bfloat16,  # saves memory, works on GPU\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Example text input\n",
        "input_text = \"Complete the sentence. The quick brown fox jumps over the lazy \"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate output (limited to 50 tokens)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# Decode output\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Input:\", input_text)\n",
        "print(\"Output:\", decoded_output)\n"
      ],
      "metadata": {
        "id": "wBfXfwb9WrzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask flask-ngrok transformers torch -q\n",
        "!pip install flask-ngrok\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "4BYbzZNJ7EU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8394584"
      },
      "source": [
        "# Task\n",
        "Replace `flask-ngrok` with `pyngrok` to expose the Flask application running in Google Colab via a public ngrok URL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c082a25b"
      },
      "source": [
        "## Install pyngrok\n",
        "\n",
        "### Subtask:\n",
        "Add a cell to install the `pyngrok` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eadb890"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `pyngrok` library. This can be done using pip in a new code cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ea4ec9a"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92440aa4"
      },
      "source": [
        "## Modify flask app\n",
        "\n",
        "### Subtask:\n",
        "Update the Flask application cell to remove `flask-ngrok` and integrate `pyngrok` to establish the tunnel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eeb647c"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the Flask application code to use `pyngrok` instead of `flask-ngrok` to expose the application. This involves removing the old import and function call, importing `ngrok` from `pyngrok`, connecting to the default Flask port (5000) using `ngrok.connect()`, and printing the public URL.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7111b668"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to connect with ngrok failed because it requires authentication. To resolve this, I need to add the ngrok authtoken before connecting. I will use the `ngrok.set_auth_token()` function for this purpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "136f84d8"
      },
      "source": [
        "## Run the flask app with pyngrok\n",
        "\n",
        "### Subtask:\n",
        "Execute the modified Flask app cell to start the server and the ngrok tunnel using pyngrok.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc542d99"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the modified Flask app cell to start the server and the ngrok tunnel. The instructions indicate to run the cell containing the modified Flask application.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ed70d8a"
      },
      "source": [
        "## Run the flask app with pyngrok\n",
        "\n",
        "### Subtask:\n",
        "Execute the modified Flask app cell to start the server and the ngrok tunnel using pyngrok.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21998ac9"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the cell containing the modified Flask application code to start the server and the ngrok tunnel using pyngrok.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask pyngrok"
      ],
      "metadata": {
        "id": "KNekZqVhBZCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors"
      ],
      "metadata": {
        "id": "beKOB3-fEM-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install flask flask-cors pyngrok transformers torch --quiet\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "print(\"Loading Nemotron-Nano-9B-v2...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "# -------------------\n",
        "# Function: Two-step reasoning generation\n",
        "# -------------------\n",
        "def generate_with_reasoning(user_input, max_thinking_tokens=32, max_total_tokens=128, temperature=0.7, top_p=0.9):\n",
        "    \"\"\"\n",
        "    Approximates Hugging Face's ThinkingBudgetClient:\n",
        "    1. Generate reasoning trace first\n",
        "    2. Generate final answer conditioned on reasoning\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Generate reasoning content (short)\n",
        "    reasoning_prompt = f\"You are a helpful AI assistant. /think\\nUser: {user_input}\\nAssistant:\"\n",
        "\n",
        "    reasoning_inputs = tokenizer(reasoning_prompt, return_tensors=\"pt\").to(device)\n",
        "    reasoning_outputs = model.generate(\n",
        "        **reasoning_inputs,\n",
        "        max_new_tokens=max_thinking_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    reasoning_text = tokenizer.decode(reasoning_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Ensure reasoning text ends cleanly with </think>\n",
        "    if \"</think>\" not in reasoning_text:\n",
        "        reasoning_text += \"\\n</think>\"\n",
        "\n",
        "    # 2. Generate final answer conditioned on reasoning\n",
        "    final_prompt = f\"{reasoning_text}\\nUser: {user_input}\\nAssistant:\"\n",
        "    final_inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
        "    remaining_tokens = max_total_tokens - final_inputs.input_ids.shape[1]\n",
        "\n",
        "    final_outputs = model.generate(\n",
        "        **final_inputs,\n",
        "        max_new_tokens=remaining_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    final_text = tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Strip out the reasoning trace from final output\n",
        "    # Keep only text after </think> or the last 'Assistant:' line\n",
        "    if \"</think>\" in final_text:\n",
        "        final_text = final_text.split(\"</think>\")[-1].strip()\n",
        "    elif \"Assistant:\" in final_text:\n",
        "        final_text = final_text.split(\"Assistant:\")[-1].strip()\n",
        "\n",
        "    return final_text\n",
        "\n",
        "# -------------------\n",
        "# Flask endpoint\n",
        "# -------------------\n",
        "@app.route(\"/api/respond\", methods=[\"POST\"])\n",
        "def respond():\n",
        "    data = request.json\n",
        "    user_input = data.get(\"input\", \"\")\n",
        "\n",
        "    output_text = generate_with_reasoning(user_input)\n",
        "    return jsonify({\"input\": user_input, \"output\": output_text})\n",
        "\n",
        "# -------------------\n",
        "# ngrok tunnel\n",
        "# -------------------\n",
        "NGROK_AUTH_TOKEN = \"34oJGiQHYcc8THKsPzsqUXBLMi6_5oBmkk9dHcsN2T2S5W2gG\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = ngrok.connect(5000).public_url\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# -------------------\n",
        "# Run server\n",
        "# -------------------\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "0RirriIAQT1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Clear the CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_summary())\n",
        "# Delete variables that might be holding onto GPU memory\n",
        "# You might need to add other variable names here if you have more objects on the GPU\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if 'pipe' in locals():\n",
        "    del pipe\n",
        "if 'pipe_smaller' in locals():\n",
        "    del pipe_smaller\n",
        "if 'input_ids' in locals():\n",
        "    del input_ids\n",
        "if 'outputs' in locals():\n",
        "    del outputs\n",
        "if 'tokenizer' in locals():\n",
        "    del tokenizer\n",
        "\n",
        "# Run Python's garbage collector\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"GPU memory hopefully cleared.\")\n",
        "\n",
        "# Choose device (this variable is not used in pipeline after fixing the error)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/Llama-3.1-Nemotron-Nano-4B-v1.1\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "# Create a pipeline for simplicity\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    # Removed the device=device argument as recommended by the error\n",
        ")\n",
        "\n",
        "# Function to complete a sentence\n",
        "def complete_sentence(prompt: str, max_new_tokens: int = 50) -> str:\n",
        "    outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "      repetition_penalty=1.15,\n",
        "      pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # The pipeline output format might differ slightly from model.generate\n",
        "    # Adjust decoding if necessary based on pipeline's return\n",
        "    return outputs[0][\"generated_text\"]\n",
        "\n",
        "# Example usage\n",
        "prompt = \"The quick brown fox jumped over the\"\n",
        "result = complete_sentence(prompt, max_new_tokens=10)\n",
        "print(\"Result:\", result)"
      ],
      "metadata": {
        "id": "U6IBkDQ3UTUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7776bbcc"
      },
      "source": [
        "# Clear the CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Delete variables that might be holding onto GPU memory\n",
        "# You might need to add other variable names here if you have more objects on the GPU\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if 'pipe' in locals():\n",
        "    del pipe\n",
        "if 'pipe_smaller' in locals():\n",
        "    del pipe_smaller\n",
        "if 'input_ids' in locals():\n",
        "    del input_ids\n",
        "if 'outputs' in locals():\n",
        "    del outputs\n",
        "if 'tokenizer' in locals():\n",
        "    del tokenizer\n",
        "\n",
        "# Run Python's garbage collector\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"GPU memory hopefully cleared.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3931a8"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "## Data Analysis Key Findings\n",
        "\n",
        "*   The `pyngrok` library was successfully installed.\n",
        "*   Attempts to establish an ngrok tunnel using `pyngrok` failed due to an authentication error (`ERR_NGROK_4018`).\n",
        "*   The authentication failure occurred because the `NGROK_AUTH_TOKEN` environment variable was not set in the Google Colab environment.\n",
        "*   The Flask application server did not start successfully as the script terminated after the `ngrok.connect()` call failed.\n",
        "\n",
        "## Insights or Next Steps\n",
        "\n",
        "*   To successfully expose the Flask application via a public ngrok URL, the user must set the `NGROK_AUTH_TOKEN` environment variable in their Google Colab environment or secrets with a valid ngrok authentication token.\n",
        "*   After setting the authtoken, re-executing the Flask application cell is required to attempt establishing the ngrok tunnel and starting the Flask server.\n"
      ]
    }
  ]
}