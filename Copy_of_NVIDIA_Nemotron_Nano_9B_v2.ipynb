{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylerle29/Memotron/blob/main/Copy_of_NVIDIA_Nemotron_Nano_9B_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_yJ572JWry_",
        "outputId": "e1fbecb4-4bfc-4ada-9a6b-de9661268895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (4.53.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (2.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.33.1\n",
            "    Uninstalling huggingface-hub-0.33.1:\n",
            "      Successfully uninstalled huggingface-hub-0.33.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.0\n",
            "    Uninstalling transformers-4.53.0:\n",
            "      Successfully uninstalled transformers-4.53.0\n",
            "Successfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers\n",
        "\n",
        "trust_remote_code=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12770087",
        "outputId": "4820d659-134b-4680-d38f-9768af211dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mamba-ssm\n",
            "  Using cached mamba_ssm-2.2.6.post3.tar.gz (113 kB)\n",
            "  Installing build dependencies ... \u001b[?25l/^C\n",
            "Traceback (most recent call last):\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/bin/pip\"\u001b[0m, line \u001b[35m11\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
            "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_internal/cli/main.py\"\u001b[0m, line \u001b[35m80\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "    return \u001b[31mcommand.main\u001b[0m\u001b[1;31m(cmd_args)\u001b[0m\n",
            "           \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_internal/cli/base_command.py\"\u001b[0m, line \u001b[35m157\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "    with \u001b[31mself.main_context\u001b[0m\u001b[1;31m()\u001b[0m:\n",
            "         \u001b[31m~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m148\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    \u001b[31mnext\u001b[0m\u001b[1;31m(self.gen)\u001b[0m\n",
            "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_internal/cli/command_context.py\"\u001b[0m, line \u001b[35m19\u001b[0m, in \u001b[35mmain_context\u001b[0m\n",
            "    with \u001b[1;31mself._main_context\u001b[0m:\n",
            "         \u001b[1;31m^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m619\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    raise exc\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m604\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    if \u001b[31mcb\u001b[0m\u001b[1;31m(*exc_details)\u001b[0m:\n",
            "       \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m148\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    \u001b[31mnext\u001b[0m\u001b[1;31m(self.gen)\u001b[0m\n",
            "    \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m42\u001b[0m, in \u001b[35mglobal_tempdir_manager\u001b[0m\n",
            "    with \u001b[31mExitStack\u001b[0m\u001b[1;31m()\u001b[0m as stack:\n",
            "         \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m619\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    raise exc\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/contextlib.py\"\u001b[0m, line \u001b[35m604\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    if \u001b[31mcb\u001b[0m\u001b[1;31m(*exc_details)\u001b[0m:\n",
            "       \u001b[31m~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m169\u001b[0m, in \u001b[35m__exit__\u001b[0m\n",
            "    \u001b[31mself.cleanup\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_internal/utils/temp_dir.py\"\u001b[0m, line \u001b[35m212\u001b[0m, in \u001b[35mcleanup\u001b[0m\n",
            "    \u001b[31mrmtree\u001b[0m\u001b[1;31m(self._path, ignore_errors=False)\u001b[0m\n",
            "    \u001b[31m~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_internal/utils/retry.py\"\u001b[0m, line \u001b[35m34\u001b[0m, in \u001b[35mretry_wrapped\u001b[0m\n",
            "    return func(*args, **kwargs)\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_internal/utils/misc.py\"\u001b[0m, line \u001b[35m136\u001b[0m, in \u001b[35mrmtree\u001b[0m\n",
            "    \u001b[31mshutil.rmtree\u001b[0m\u001b[1;31m(dir, onexc=handler)\u001b[0m  # type: ignore\n",
            "    \u001b[31m~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/shutil.py\"\u001b[0m, line \u001b[35m763\u001b[0m, in \u001b[35mrmtree\u001b[0m\n",
            "    \u001b[31m_rmtree_safe_fd\u001b[0m\u001b[1;31m(stack, onexc)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^\u001b[0m\n",
            "  File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/shutil.py\"\u001b[0m, line \u001b[35m696\u001b[0m, in \u001b[35m_rmtree_safe_fd\u001b[0m\n",
            "    \u001b[31mos.unlink\u001b[0m\u001b[1;31m(entry.name, dir_fd=topfd)\u001b[0m\n",
            "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "\u001b[1;35mKeyboardInterrupt\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -U mamba-ssm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KyZzeJUWrzA"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wBfXfwb9WrzB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\n# Load tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\\n\\n# Load model (on GPU if available)\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\\n    torch_dtype=torch.bfloat16,  # saves memory, works on GPU\\n    trust_remote_code=True,\\n    device_map=\"auto\"\\n)\\n\\n# Example text input\\ninput_text = \"Hello, how are you today?\"\\n\\n# Tokenize input\\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\\n\\n# Generate output (limited to 50 tokens)\\noutputs = model.generate(**inputs, max_new_tokens=50)\\n\\n# Decode output\\ndecoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\nprint(\"Input:\", input_text)\\nprint(\"Output:\", decoded_output)\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\n",
        "\n",
        "# Load model (on GPU if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
        "    torch_dtype=torch.bfloat16,  # saves memory, works on GPU\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Example text input\n",
        "input_text = \"Hello, how are you today?\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate output (limited to 50 tokens)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# Decode output\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Input:\", input_text)\n",
        "print(\"Output:\", decoded_output)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4BYbzZNJ7EU-"
      },
      "outputs": [],
      "source": [
        "!pip install flask flask-ngrok transformers torch -q\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "dXzhl4zG7U-X",
        "outputId": "9df866d3-b94a-4ec7-84bd-ee8573431225"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "mamba-ssm is required by the Mamba model but cannot be imported",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/nvidia/NVIDIA_hyphen_Nemotron_hyphen_Nano_hyphen_9B_hyphen_v2/dbe2b5b379f25ec52223b39e2c097d3e9654b8db/modeling_nemotron_h.py:63\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m#from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayernorm_gated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rmsnorm_fn\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mamba_ssm'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mnvidia/NVIDIA-Nemotron-Nano-9B-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnvidia/NVIDIA-Nemotron-Nano-9B-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Flask route for React frontend\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@app\u001b[39m.route(\u001b[33m\"\u001b[39m\u001b[33m/api/respond\u001b[39m\u001b[33m\"\u001b[39m, methods=[\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrespond\u001b[39m():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:586\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33madapter_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = adapter_kwargs\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     model_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m     _ = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    590\u001b[39m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[32m    591\u001b[39m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[32m    592\u001b[39m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:616\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m    604\u001b[39m final_module = get_cached_module_file(\n\u001b[32m    605\u001b[39m     repo_id,\n\u001b[32m    606\u001b[39m     module_file + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m     repo_type=repo_type,\n\u001b[32m    615\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:311\u001b[39m, in \u001b[36mget_class_in_module\u001b[39m\u001b[34m(class_name, module_path, force_reload)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m__transformers_module_hash__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) != module_hash:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[43mmodule_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     module.__transformers_module_hash__ = module_hash\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/nvidia/NVIDIA_hyphen_Nemotron_hyphen_Nano_hyphen_9B_hyphen_v2/dbe2b5b379f25ec52223b39e2c097d3e9654b8db/modeling_nemotron_h.py:65\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayernorm_gated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rmsnorm_fn\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmamba-ssm is required by the Mamba model but cannot be imported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_causal_conv1d_available():\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcausal_conv1d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m causal_conv1d_fn, causal_conv1d_update\n",
            "\u001b[31mImportError\u001b[39m: mamba-ssm is required by the Mamba model but cannot be imported"
          ]
        }
      ],
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # exposes public URL\n",
        "\n",
        "# -------------------------------\n",
        "# Load Nemotron-Nano-9B-v2\n",
        "# -------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Flask route for React frontend\n",
        "# -------------------------------\n",
        "@app.route(\"/api/respond\", methods=[\"POST\"])\n",
        "def respond():\n",
        "    data = request.json\n",
        "    input_text = data.get(\"input\", \"\")\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output (50 tokens max)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "    # Decode output\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return jsonify({\n",
        "        \"input\": input_text,\n",
        "        \"output\": decoded_output\n",
        "    })\n",
        "\n",
        "# -------------------------------\n",
        "# Run Flask app\n",
        "# -------------------------------\n",
        "app.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7776bbcc"
      },
      "outputs": [],
      "source": [
        "# Clear the CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Delete variables that might be holding onto GPU memory\n",
        "# You might need to add other variable names here if you have more objects on the GPU\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if 'pipe' in locals():\n",
        "    del pipe\n",
        "if 'pipe_smaller' in locals():\n",
        "    del pipe_smaller\n",
        "if 'input_ids' in locals():\n",
        "    del input_ids\n",
        "if 'outputs' in locals():\n",
        "    del outputs\n",
        "if 'tokenizer' in locals():\n",
        "    del tokenizer\n",
        "\n",
        "# Run Python's garbage collector\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"GPU memory hopefully cleared.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "memeatron",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
