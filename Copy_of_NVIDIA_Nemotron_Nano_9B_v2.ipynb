{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylerle29/Memotron/blob/main/Copy_of_NVIDIA_Nemotron_Nano_9B_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_yJ572JWry_",
        "outputId": "e1fbecb4-4bfc-4ada-9a6b-de9661268895"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (2.3.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers\n",
        "\n",
        "trust_remote_code=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12770087",
        "outputId": "4820d659-134b-4680-d38f-9768af211dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mamba-ssm\n",
            "  Using cached mamba_ssm-2.2.6.post3.tar.gz (113 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[28 lines of output]\u001b[0m\n",
            "  \u001b[31m   \u001b[0m /private/var/folders/4w/pcwd3yms7fq5zgcn49lgctcr0000gn/T/pip-build-env-vem382mc/overlay/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
            "  \u001b[31m   \u001b[0m   cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m torch.__version__  = 2.9.0\n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m \n",
            "  \u001b[31m   \u001b[0m <string>:118: UserWarning: mamba_ssm was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
            "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
            "  \u001b[31m   \u001b[0m                              \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     return hook(config_settings)\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/4w/pcwd3yms7fq5zgcn49lgctcr0000gn/T/pip-build-env-vem382mc/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
            "  \u001b[31m   \u001b[0m            \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/4w/pcwd3yms7fq5zgcn49lgctcr0000gn/T/pip-build-env-vem382mc/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"/private/var/folders/4w/pcwd3yms7fq5zgcn49lgctcr0000gn/T/pip-build-env-vem382mc/overlay/lib/python3.13/site-packages/setuptools/build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
            "  \u001b[31m   \u001b[0m     \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
            "  \u001b[31m   \u001b[0m   File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m175\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[1;35mNameError\u001b[0m: \u001b[35mname 'bare_metal_version' is not defined\u001b[0m\n",
            "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U mamba-ssm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KyZzeJUWrzA"
      },
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2\n",
        "\n",
        "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wBfXfwb9WrzB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\n# Load tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\\n\\n# Load model (on GPU if available)\\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\\n    torch_dtype=torch.bfloat16,  # saves memory, works on GPU\\n    trust_remote_code=True,\\n    device_map=\"auto\"\\n)\\n\\n# Example text input\\ninput_text = \"Hello, how are you today?\"\\n\\n# Tokenize input\\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\\n\\n# Generate output (limited to 50 tokens)\\noutputs = model.generate(**inputs, max_new_tokens=50)\\n\\n# Decode output\\ndecoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\\nprint(\"Input:\", input_text)\\nprint(\"Output:\", decoded_output)\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\")\n",
        "\n",
        "# Load model (on GPU if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/NVIDIA-Nemotron-Nano-9B-v2\",\n",
        "    torch_dtype=torch.bfloat16,  # saves memory, works on GPU\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Example text input\n",
        "input_text = \"Hello, how are you today?\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate output (limited to 50 tokens)\n",
        "outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# Decode output\n",
        "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Input:\", input_text)\n",
        "print(\"Output:\", decoded_output)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4BYbzZNJ7EU-"
      },
      "outputs": [],
      "source": [
        "!pip install flask flask-ngrok transformers torch -q\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "dXzhl4zG7U-X",
        "outputId": "9df866d3-b94a-4ec7-84bd-ee8573431225"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "mamba-ssm is required by the Mamba model but cannot be imported",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/nvidia/NVIDIA_hyphen_Nemotron_hyphen_Nano_hyphen_9B_hyphen_v2/dbe2b5b379f25ec52223b39e2c097d3e9654b8db/modeling_nemotron_h.py:63\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m#from mamba_ssm.ops.triton.layernorm_gated import RMSNorm as RMSNormGated\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayernorm_gated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rmsnorm_fn\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mamba_ssm'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mnvidia/NVIDIA-Nemotron-Nano-9B-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnvidia/NVIDIA-Nemotron-Nano-9B-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Flask route for React frontend\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@app\u001b[39m.route(\u001b[33m\"\u001b[39m\u001b[33m/api/respond\u001b[39m\u001b[33m\"\u001b[39m, methods=[\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrespond\u001b[39m():\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:586\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33madapter_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = adapter_kwargs\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     model_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m     _ = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    590\u001b[39m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[32m    591\u001b[39m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[32m    592\u001b[39m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:616\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m    604\u001b[39m final_module = get_cached_module_file(\n\u001b[32m    605\u001b[39m     repo_id,\n\u001b[32m    606\u001b[39m     module_file + \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    614\u001b[39m     repo_type=repo_type,\n\u001b[32m    615\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_class_in_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/memeatron/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:311\u001b[39m, in \u001b[36mget_class_in_module\u001b[39m\u001b[34m(class_name, module_path, force_reload)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;66;03m# reload in both cases, unless the module is already imported and the hash hits\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m__transformers_module_hash__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) != module_hash:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[43mmodule_spec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexec_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     module.__transformers_module_hash__ = module_hash\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, class_name)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/nvidia/NVIDIA_hyphen_Nemotron_hyphen_Nano_hyphen_9B_hyphen_v2/dbe2b5b379f25ec52223b39e2c097d3e9654b8db/modeling_nemotron_h.py:65\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmamba_ssm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtriton\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayernorm_gated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rmsnorm_fn\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmamba-ssm is required by the Mamba model but cannot be imported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_causal_conv1d_available():\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcausal_conv1d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m causal_conv1d_fn, causal_conv1d_update\n",
            "\u001b[31mImportError\u001b[39m: mamba-ssm is required by the Mamba model but cannot be imported"
          ]
        }
      ],
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # exposes public URL\n",
        "\n",
        "# -------------------------------\n",
        "# Load Nemotron-Nano-9B-v2\n",
        "# -------------------------------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-4Bâ€‘v1.1\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"nvidia/NVIDIA-Nemotron-Nano-4B-v1.1\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Flask route for React frontend\n",
        "# -------------------------------\n",
        "@app.route(\"/api/respond\", methods=[\"POST\"])\n",
        "def respond():\n",
        "    data = request.json\n",
        "    input_text = data.get(\"input\", \"\")\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output (50 tokens max)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "    # Decode output\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return jsonify({\n",
        "        \"input\": input_text,\n",
        "        \"output\": decoded_output\n",
        "    })\n",
        "\n",
        "# -------------------------------\n",
        "# Run Flask app\n",
        "# -------------------------------\n",
        "app.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7776bbcc"
      },
      "outputs": [],
      "source": [
        "# Clear the CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Delete variables that might be holding onto GPU memory\n",
        "# You might need to add other variable names here if you have more objects on the GPU\n",
        "if 'model' in locals():\n",
        "    del model\n",
        "if 'pipe' in locals():\n",
        "    del pipe\n",
        "if 'pipe_smaller' in locals():\n",
        "    del pipe_smaller\n",
        "if 'input_ids' in locals():\n",
        "    del input_ids\n",
        "if 'outputs' in locals():\n",
        "    del outputs\n",
        "if 'tokenizer' in locals():\n",
        "    del tokenizer\n",
        "\n",
        "# Run Python's garbage collector\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "print(\"GPU memory hopefully cleared.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "memeatron",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
